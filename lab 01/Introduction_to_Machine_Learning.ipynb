{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is AutoML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn has convenient modules to create sample data.\n",
    "# make_blobs will help us to create a sample data set suitable for clustering\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=100, centers=2, cluster_std=0.30, random_state=0)\n",
    "\n",
    "# Let's visualize what we have first\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will import KMeans model from clustering model family of Sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k_means = KMeans(n_clusters=2)\n",
    "k_means.fit(X)\n",
    "predictions = k_means.predict(X)\n",
    "\n",
    "# Let's plot the predictions\n",
    "plt.scatter(X[:, 0], X[:, 1], c=predictions, cmap='brg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featuretools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# First dataset contains the basic information for databases.\n",
    "databases_df = pd.DataFrame({\"database_id\": [2234, 1765, 8796, 2237, 3398],\n",
    "\"creation_date\": [\"2018-02-01\", \"2017-03-02\", \"2017-05-03\", \"2013-05-12\", \"2012-05-09\"]})\n",
    "\n",
    "databases_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second dataset contains the information of transaction for each database id\n",
    "db_transactions_df = pd.DataFrame({\"transaction_id\": [26482746, 19384752, 48571125, 78546789, 19998765, 26482646, 12484752, 42471125, 75346789, 16498765, 65487547, 23453847, 56756771, 45645667, 23423498, 12335268, 76435357, 34534711, 45656746, 12312987],\n",
    "                \"database_id\": [2234, 1765, 2234, 2237, 1765, 8796, 2237, 8796, 3398, 2237, 3398, 2237, 2234, 8796, 1765, 2234, 2237, 1765, 8796, 2237],\n",
    "                \"transaction_size\": [10, 20, 30, 50, 100, 40, 60, 60, 10, 20, 60, 50, 40, 40, 30, 90, 130, 40, 50, 30],\n",
    "                \"transaction_date\": [\"2018-02-02\", \"2018-03-02\", \"2018-03-02\", \"2018-04-02\", \"2018-04-02\", \"2018-05-02\", \"2018-06-02\", \"2018-06-02\", \"2018-07-02\", \"2018-07-02\", \"2018-01-03\", \"2018-02-03\", \"2018-03-03\", \"2018-04-03\", \"2018-04-03\", \"2018-07-03\", \"2018-07-03\", \"2018-07-03\", \"2018-08-03\", \"2018-08-03\"]})\n",
    "\n",
    "db_transactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entities for each of datasets should be defined\n",
    "entities = {\n",
    "\"databases\" : (databases_df, \"database_id\"),\n",
    "\"transactions\" : (db_transactions_df, \"transaction_id\")\n",
    "}\n",
    "\n",
    "# Relationships between tables should also be defined as below\n",
    "relationships = [(\"databases\", \"database_id\", \"transactions\", \"database_id\")]\n",
    "\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 2 entities called ‘databases’ and ‘transactions’\n",
    "# All the pieces that are necessary to engineer features are in place, you can create your feature matrix as below\n",
    "\n",
    "import featuretools as ft\n",
    "\n",
    "feature_matrix_db_transactions, feature_defs = ft.dfs(entities=entities, relationships=relationships, target_entity=\"databases\")\n",
    "\n",
    "feature_defs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import autosklearn.classification\n",
    "import sklearn.model_selection\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Digits dataset is one of the most popular datasets in machine learning community.\n",
    "# Every example in this datasets represents a 8x8 image of a digit.\n",
    "X, y = sklearn.datasets.load_digits(return_X_y=True)\n",
    "\n",
    "# Let's see the first image. Image is reshaped to 8x8, otherwise it's a vector of size 64.\n",
    "X[0].reshape(8,8)\n",
    "\n",
    "# Let's also plot couple of them\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "number_of_images = 10\n",
    "images_and_labels = list(zip(X, y))\n",
    "\n",
    "for i, (image, label) in enumerate(images_and_labels[:number_of_images]):\n",
    "    plt.subplot(2, number_of_images, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image.reshape(8,8), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('%i' % label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split our dataset to train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Similarly to creating an estimator in Scikit-learn, we create AutoSklearnClassifier\n",
    "automl = autosklearn.classification.AutoSklearnClassifier()\n",
    "\n",
    "# All you need to do is to invoke fit method to start experiment with different feature engineering methods and machine learning models\n",
    "automl.fit(X_train, y_train)\n",
    "\n",
    "# Generating predictions is same as Scikit-learn, you need to invoke predict method.\n",
    "y_hat = automl.predict(X_test)\n",
    "\n",
    "print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_hat))\n",
    "# Accuracy score 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "from mlbox.preprocessing import *\n",
    "from mlbox.optimisation import *\n",
    "from mlbox.prediction import *\n",
    "import wget\n",
    "\n",
    "file_link = 'https://apsportal.ibm.com/exchange-api/v1/entries/8044492073eb964f46597b4be06ff5ea/data?accessKey=9561295fa407698694b1e254d0099600'\n",
    "file_name = wget.download(file_link)\n",
    "\n",
    "print(file_name)\n",
    "# GoSales_Tx_NaiveBayes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('GoSales_Tx_NaiveBayes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.drop(['PRODUCT_LINE'], axis = 1)\n",
    "\n",
    "# First 300 records saved as test datased\n",
    "test_df[:300].to_csv('test_data.csv')\n",
    "\n",
    "paths = [\"GoSales_Tx_NaiveBayes.csv\", \"test_data.csv\"]\n",
    "target_name = \"PRODUCT_LINE\"\n",
    "\n",
    "rd = Reader(sep = ',')\n",
    "df = rd.train_test_split(paths, target_name)\n",
    "\n",
    "dft = Drift_thresholder()\n",
    "df = dft.fit_transform(df)\n",
    "\n",
    "opt = Optimiser(scoring = 'accuracy', n_folds = 3)\n",
    "opt.evaluate(None, df)\n",
    "\n",
    "space = {\n",
    "        'ne__numerical_strategy':{\"search\":\"choice\", \"space\":[0]},\n",
    "        'ce__strategy':{\"search\":\"choice\",\n",
    "               \"space\":[\"label_encoding\",\"random_projection\", \"entity_embedding\"]},\n",
    "        'fs__threshold':{\"search\":\"uniform\", \"space\":[0.01,0.3]},\n",
    "        'est__max_depth':{\"search\":\"choice\", \"space\":[3,4,5,6,7]}\n",
    "        }\n",
    "\n",
    "best = opt.optimise(space, df,15)\n",
    "\n",
    "predictor = Predictor()\n",
    "predictor.fit_predict(best, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Digits dataset that you have used in Auto-sklearn example\n",
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,\n",
    "                                                    train_size=0.75, test_size=0.25)\n",
    "\n",
    "# You will create your TPOT classifier with commonly used arguments\n",
    "tpot = TPOTClassifier(generations=10, population_size=30, verbosity=2)\n",
    "\n",
    "# When you invoke fit method, TPOT will create generations of populations, seeking best set of parameters. Arguments you have used to create TPOTClassifier such as generaions and population_size will affect the search space and resulting pipeline.\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "print(tpot.score(X_test, y_test))\n",
    "# 0.9834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.export('my_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat my_pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=DecisionTreeClassifier(criterion=\"entropy\", max_depth=6, min_samples_leaf=2, min_samples_split=2)),\n",
    "    KNeighborsClassifier(n_neighbors=2, weights=\"distance\")\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(X_train, y_train)\n",
    "results = exported_pipeline.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
