{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# Setting the style of the plot\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Creating an array of input sizes\n",
    "n = 10\n",
    "x = np.arange(1, n)\n",
    "\n",
    "# Creating a pandas data frame for popular complexity classes\n",
    "df = pd.DataFrame({'x': x,\n",
    "                   'O(1)': 0,\n",
    "                   'O(n)': x,\n",
    "                   'O(log_n)': np.log(x),\n",
    "                   'O(n_log_n)': n * np.log(x),\n",
    "                   'O(n2)': np.power(x, 2), # Quadratic\n",
    "                   'O(n3)': np.power(x, 3)}) # Cubic\n",
    "\n",
    "# Creating labels\n",
    "labels = ['$O(1) - Constant$',\n",
    "          '$O(\\log{}n) - Logarithmic$',\n",
    "          '$O(n) - Linear$',\n",
    "          '$O(n^2) - Quadratic$',\n",
    "          '$O(n^3) - Cubic$',\n",
    "          '$O(n\\log{}n) - N log n$']\n",
    "\n",
    "# Plotting every column in dataframe except 'x'\n",
    "for i, col in enumerate(df.columns.drop('x')):\n",
    "    print(labels[i], col)\n",
    "    plt.plot(df[col], label=labels[i])\n",
    "\n",
    "# Adding a legend\n",
    "plt.legend()\n",
    "\n",
    "# Limiting the y-axis\n",
    "plt.ylim(0,50)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple measure of training and scoring time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from time import time\n",
    "\n",
    "@contextmanager\n",
    "def timer():\n",
    "    s = time()\n",
    "    yield\n",
    "    e = time() - s\n",
    "    print(\"{0}: {1} ms\".format('Elapsed time', e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with timer():\n",
    "    X = np.random.rand(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Defining properties of dataset\n",
    "nT = 100000000 # Total number of values in our dataset\n",
    "nF = 10 # Number of features\n",
    "nE = int(nT / nF) # Number of examples\n",
    "\n",
    "# Creating n x m matrix where n=100 and m=10\n",
    "X = np.random.rand(nT).reshape(nE, nF)\n",
    "\n",
    "# This will be a binary classification with labels 0 and 1\n",
    "y = np.random.randint(2, size=nE)\n",
    "\n",
    "# Data that we are going to score\n",
    "scoring_data = np.random.rand(nF).reshape(1,-1)\n",
    "\n",
    "# Create KNN classifier\n",
    "knn = KNeighborsClassifier(11, algorithm='brute')\n",
    "\n",
    "# Measure training time\n",
    "with timer():\n",
    "    knn.fit(X, y)\n",
    "\n",
    "# Measure scoring time\n",
    "with timer():\n",
    "    knn.predict(scoring_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_res = LogisticRegression(C=1e5)\n",
    "\n",
    "with timer():\n",
    "    log_res.fit(X, y)\n",
    "\n",
    "with timer():\n",
    "    prediction = log_res.predict(scoring_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code profiling in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cProfile\n",
    "import cProfile\n",
    "\n",
    "cProfile.run('np.std(np.random.rand(1000000))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following code snippet should be put into profiler_example_1.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.std(np.random.rand(1000000))\n",
    "\n",
    "#### Then from your terminal you should run the following command\n",
    "\n",
    "python -m cProfile -o profiler_output -s cumulative profiler_example_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing performance statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To visualize results, you should run the following command from your terminal\n",
    "\n",
    "snakeviz profiler_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbours from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save below code block as knn_prediction.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "# distance module includes various distance functions\n",
    "# You will use euclidean distance function to calculate distances between scoring input and training dataset.\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Decorating function with @profile to get run statistics\n",
    "@profile\n",
    "def nearest_neighbors_prediction(x, data, labels, k):\n",
    "\n",
    "    # Euclidean distance will be calculated between example to be predicted and examples in data\n",
    "    distances = np.array([distance.euclidean(x, i) for i in data])\n",
    "\n",
    "    label_count = {}\n",
    "    for i in range(k):\n",
    "        # Sorting distances starting from closest to our example\n",
    "        label = labels[distances.argsort()[i]]\n",
    "        label_count[label] = label_count.get(label, 0) + 1\n",
    "    votes = sorted(label_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    # Return the majority vote\n",
    "    return votes[0][0]\n",
    "\n",
    "# Setting seed to make results reproducible\n",
    "np.random.seed(23)\n",
    "\n",
    "# Creating dataset, 20 x 5 matrix which means 20 examples with 5 features for each\n",
    "data = np.random.rand(100).reshape(20,5)\n",
    "\n",
    "# Creating labels\n",
    "labels = np.random.choice(2, 20)\n",
    "\n",
    "# Scoring input\n",
    "x = np.random.rand(5)\n",
    "\n",
    "# Predicting class for scoring input with k=2\n",
    "pred = nearest_neighbors_prediction(x, data, labels, k=2)\n",
    "# Output is ‘0’ in my case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling your Python script line by line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the following command from your terminal\n",
    "\n",
    "kernprof -l knn_prediction.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To see the results, run the following command from your terminal\n",
    "\n",
    "python -m line_profiler knn_prediction.py.lprof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearity versus non-linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "# This function will scale training datatse and train given classifier.\n",
    "# Based on predictions it will draw decision boundaries.\n",
    "\n",
    "def draw_decision_boundary(clf, X, y, h = .01, figsize=(9,9), boundary_cmap=cm.winter, points_cmap=cm.cool):\n",
    "\n",
    "    # After you apply StandardScaler, feature means will be removed and all features will have unit variance.\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Splitting dataset to train and test sets.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "    # Training given estimator on training dataset by invoking fit function.\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Each estimator has a score function.\n",
    "    # Score will show you estimator's performance by showing metric suitable to given classifier.\n",
    "    # For example, for linear regression, it will output coefficient of determination R^2 of the prediction.\n",
    "    # For logistic regression, it will output mean accuracy.\n",
    "\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print(\"Score: %0.3f\" % score)\n",
    "\n",
    "    # Predict function of an estimator, will predict using trained model\n",
    "    pred = clf.predict(X_test)\n",
    "\n",
    "    # Figure is a high-level container that contains plot elements\n",
    "    figure = plt.figure(figsize=figsize)\n",
    "\n",
    "    # In current figure, subplot will create Axes based on given arguments (nrows, ncols, index)\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "\n",
    "    # Calculating min/max of axes\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "    # Meshgrid is usually used to evaluate function on grid.\n",
    "    # It will allow you to create points to represent the space you operate\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Generate predictions for all the point-pair created by meshgrid\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # This will draw boundary\n",
    "    ax.contourf(xx, yy, Z, cmap=boundary_cmap)\n",
    "\n",
    "    # Plotting training data\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=points_cmap, edgecolors='k')\n",
    "\n",
    "    # Potting testing data\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=points_cmap, alpha=0.6, edgecolors='k')\n",
    "\n",
    "    # Showing your masterpiece\n",
    "    figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "# sklearn.linear_model includes regression models where target variable is a linear combination of input variables\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# make_moons is another useful function to generate sample data\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise=0.1, random_state=0)\n",
    "\n",
    "# Plot sample data\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=cm.cool)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_decision_boundary(LogisticRegression(), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "draw_decision_boundary(RandomForestClassifier(), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autosklearn.classification import AutoSklearnClassifier\n",
    "AutoSklearnClassifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preceeding command will give you the following output**\n",
    "\n",
    "Init signature: AutoSklearnClassifier(time_left_for_this_task=3600, per_run_time_limit=360, initial_configurations_via_metalearning=25, ensemble_size=50, ensemble_nbest=50, seed=1, ml_memory_limit=3072, include_estimators=None, exclude_estimators=None, include_preprocessors=None, exclude_preprocessors=None, resampling_strategy='holdout', resampling_strategy_arguments=None, tmp_folder=None, output_folder=None, delete_tmp_folder_after_terminate=True, delete_output_folder_after_terminate=True, shared_mode=False, disable_evaluator_output=False, get_smac_object_callback=None, smac_scenario_args=None)\n",
    "Docstring:      This class implements the classification task.\n",
    "Init docstring:\n",
    "\n",
    "Parameters\n",
    "\n",
    "time_left_for_this_task : int, optional (default=3600)\n",
    "    Time limit in seconds for the search of appropriate\n",
    "    models. By increasing this value, *auto-sklearn* has a higher\n",
    "    chance of finding better models.\n",
    "\n",
    "per_run_time_limit : int, optional (default=360)\n",
    "    Time limit for a single call to the machine learning model.\n",
    "    Model fitting will be terminated if the machine learning\n",
    "    algorithm runs over the time limit. Set this value high enough ...\n",
    "    ...\n",
    "    \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary variables\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autosklearn.classification import AutoSklearnClassifier\n",
    "from autosklearn.regression import AutoSklearnRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import wget\n",
    "import pandas as pd\n",
    "\n",
    "# Machine learning algorithms work with numerical inputs and you need to transform all non-numerical inputs to numerical ones\n",
    "# Following snippet encode the categorical variables\n",
    "\n",
    "link_to_data = 'https://apsportal.ibm.com/exchange-api/v1/entries/8044492073eb964f46597b4be06ff5ea/data?accessKey=9561295fa407698694b1e254d0099600'\n",
    "filename = wget.download(link_to_data)\n",
    "\n",
    "print(filename)\n",
    "# GoSales_Tx_NaiveBayes.csv\n",
    "\n",
    "df = pd.read_csv('GoSales_Tx_NaiveBayes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(LabelEncoder().fit_transform)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function below will encode the target variable if needed\n",
    "def encode_target_variable(df=None, target_column=None, y=None):\n",
    "\n",
    "    # Below section will encode target variable if given data is pandas dataframe\n",
    "    if df is not None:\n",
    "        df_type = isinstance(df, pd.core.frame.DataFrame)\n",
    "\n",
    "        # Splitting dataset as train and test data sets\n",
    "        if df_type:\n",
    "\n",
    "            # If column data type is not numeric then labels are encoded\n",
    "            if not np.issubdtype(df[target_column].dtype, np.number):\n",
    "                le = preprocessing.LabelEncoder()\n",
    "                df[target_column] = le.fit_transform(df[target_column])\n",
    "                return df[target_column]\n",
    "\n",
    "            return df[target_column]\n",
    "    # Below section will encode numpy array.\n",
    "    else:\n",
    "\n",
    "        # numpy array's data type is not numeric then labels are encoded\n",
    "        if not np.issubdtype(y.dtype, np.number):\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            y = le.fit_transform(y)\n",
    "            return y\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "# Create a wrapper function where you can specify the type of learning problem\n",
    "def supervised_learner(type, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    if type == 'regression':\n",
    "        # You can play with time related arguments for discovering more pipelines\n",
    "        automl = AutoSklearnRegressor(time_left_for_this_task=7200, per_run_time_limit=720)\n",
    "    else:\n",
    "        automl = AutoSklearnClassifier(time_left_for_this_task=7200, per_run_time_limit=720)\n",
    "\n",
    "    # Training estimator based on learner type\n",
    "    automl.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting labels on test data\n",
    "    y_hat = automl.predict(X_test)\n",
    "\n",
    "    # Calculating accuracy_score\n",
    "    metric = accuracy_score(y_test, y_hat)\n",
    "\n",
    "    # Return model, labels and metric\n",
    "    return automl, y_hat, metric\n",
    "\n",
    "# In function below, you need to provide numpy array or pandas dataframe together with the name of the target column as arguments\n",
    "def supervised_automl(data, target_column=None, type=None, y=None):\n",
    "\n",
    "    # First thing is to check wheter data is pandas dataframe\n",
    "    df_type = isinstance(data, pd.core.frame.DataFrame)\n",
    "\n",
    "    # Based on data type, you will split dataset as train and test data sets\n",
    "    if df_type:\n",
    "        # This is where encode_target_variable function is used before data split\n",
    "        data[target_column] = encode_target_variable(data, target_column)\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_test_split(data.loc[:, data.columns != target_column], data[target_column], random_state=1)\n",
    "    else:\n",
    "        y_encoded = encode_target_variable(y=y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, random_state=1)\n",
    "\n",
    "    # If learner type is given, then you invoke supervied_learner\n",
    "    if type != None:\n",
    "        automl, y_hat, metric = supervised_learner(type, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # If type of learning problem is not given, you need to infer it\n",
    "    # If there are more than 10 unique numerical values, problem will be treated as regression problem,\n",
    "    # Otherwise, classification problem\n",
    "\n",
    "    elif len(df[target_column].unique()) > 10:\n",
    "            print(\"\"\"There are more than 10 uniques numerical values in target column. \n",
    "            Treating it as regression problem.\"\"\")\n",
    "            automl, y_hat, metric = supervised_learner('regression', X_train, y_train, X_test, y_test)\n",
    "    else:\n",
    "        automl, y_hat, metric = supervised_learner('classification', X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Return model, labels and metric\n",
    "    return automl, y_hat, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl, y_hat, metric = supervised_automl(df, target_column='PRODUCT_LINE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl.get_models_with_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kddcup 99 dataset which is tcpdump portions of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset.\n",
    "# Goal is to detect network intrusions\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "\n",
    "# Downloading subset of whole dataset\n",
    "dataset = fetch_kddcup99(subset='http', shuffle=True, percent10=True)\n",
    "# Downloading https://ndownloader.figshare.com/files/5976042\n",
    "# [INFO] [17:43:19:sklearn.datasets.kddcup99] Downloading https://ndownloader.figshare.com/files/5976042\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "# 58725 examples with 3 features\n",
    "X.shape\n",
    "# (58725, 3)\n",
    "\n",
    "y.shape\n",
    "(58725,)\n",
    "\n",
    "# 5 different classes to represent network anolmaly\n",
    "pprint(np.unique(y))\n",
    "# array([b'back.', b'ipsweep.', b'normal.', b'phf.', b'satan.'], dtype=object)\n",
    "\n",
    "automl, y_hat, metric = supervised_automl(X, y=y, type='classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set context helps you to adjust things like label size, lines and various elements\n",
    "# Try \"notebook\", \"talk\" or \"paper\" instead of \"poster\" to see how it changes\n",
    "sns.set_context('poster')\n",
    "\n",
    "# set_color_codes will affect how colors such as 'r', 'b', 'g' will be interpreted\n",
    "sns.set_color_codes()\n",
    "\n",
    "# Plot keyword arguments will allow you to set things like size or line width to be used in charts.\n",
    "plot_kwargs = {'s': 10, 'linewidths': 0.1}\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pprint will better output your variables in console for readability\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# Creating sample dataset using sklearn samples_generator\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Make blobs will generate isotropic Gaussian blobs\n",
    "# You can play with arguments like center of blobs, cluster standard deviation\n",
    "centers = [[2, 1], [-1.5, -1], [1, -1], [-2, 2]]\n",
    "cluster_std = [0.1, 0.1, 0.1, 0.1]\n",
    "\n",
    "# Sample data will help you to see your algorithms behavior\n",
    "X, y = make_blobs(n_samples=1000,\n",
    "                  centers=centers,\n",
    "                  cluster_std=cluster_std,\n",
    "                  random_state=53)\n",
    "\n",
    "# Plot generated sample data\n",
    "plt.scatter(X[:, 0], X[:, 1], **plot_kwargs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_std = [0.4, 0.5, 0.6, 0.5] \n",
    "\n",
    "X, y = make_blobs(n_samples=1000,\n",
    "                  centers=centers,\n",
    "                  cluster_std=cluster_std,\n",
    "                  random_state=53)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], **plot_kwargs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unsupervised_AutoML:\n",
    "\n",
    "    def __init__(self, estimators=None, transformers=None):\n",
    "        self.estimators = estimators\n",
    "        self.transformers = transformers\n",
    "        pass\n",
    "    \n",
    "    def fit_predict(self, X, y=None):\n",
    "        \"\"\"\n",
    "        fit_predict will train given estimator(s) and predict cluster membership for each sample\n",
    "        \"\"\"\n",
    "\n",
    "        # This dictionary will hold predictions for each estimator\n",
    "        predictions = []\n",
    "        performance_metrics = {}\n",
    "\n",
    "        for estimator in self.estimators:\n",
    "            labels = estimator['estimator'](*estimator['args'], **estimator['kwargs']).fit_predict(X)\n",
    "            estimator['estimator'].n_clusters_ = len(np.unique(labels))\n",
    "            metrics = self._get_cluster_metrics(estimator['estimator'].__name__, estimator['estimator'].n_clusters_, X, labels, y)\n",
    "            predictions.append({estimator['estimator'].__name__: labels})\n",
    "            performance_metrics[estimator['estimator'].__name__] = metrics\n",
    "            \n",
    "        self.predictions = predictions\n",
    "        self.performance_metrics = performance_metrics\n",
    "\n",
    "        return predictions, performance_metrics\n",
    "    \n",
    "    # Printing cluster metrics for given arguments\n",
    "    def _get_cluster_metrics(self, name, n_clusters_, X, pred_labels, true_labels=None):\n",
    "        from sklearn.metrics import homogeneity_score, \\\n",
    "            completeness_score, \\\n",
    "            v_measure_score, \\\n",
    "            adjusted_rand_score, \\\n",
    "            adjusted_mutual_info_score, \\\n",
    "            silhouette_score\n",
    "\n",
    "        print(\"\"\"################## %s metrics #####################\"\"\" % name)\n",
    "        if len(np.unique(pred_labels)) >= 2:\n",
    "\n",
    "            silh_co = silhouette_score(X, pred_labels)\n",
    "\n",
    "            if true_labels is not None:\n",
    "\n",
    "                h_score = homogeneity_score(true_labels, pred_labels)\n",
    "                c_score = completeness_score(true_labels, pred_labels)\n",
    "                vm_score = v_measure_score(true_labels, pred_labels)\n",
    "                adj_r_score = adjusted_rand_score(true_labels, pred_labels)\n",
    "                adj_mut_info_score = adjusted_mutual_info_score(true_labels, pred_labels)\n",
    "\n",
    "                metrics = {\"Silhouette Coefficient\": silh_co,\n",
    "                           \"Estimated number of clusters\": n_clusters_,\n",
    "                           \"Homogeneity\": h_score,\n",
    "                           \"Completeness\": c_score,\n",
    "                           \"V-measure\": vm_score,\n",
    "                           \"Adjusted Rand Index\": adj_r_score,\n",
    "                           \"Adjusted Mutual Information\": adj_mut_info_score}\n",
    "\n",
    "                for k, v in metrics.items():\n",
    "                    print(\"\\t%s: %0.3f\" % (k, v))\n",
    "\n",
    "                return metrics\n",
    "\n",
    "            metrics = {\"Silhouette Coefficient\": silh_co,\n",
    "                       \"Estimated number of clusters\": n_clusters_}\n",
    "\n",
    "            for k, v in metrics.items():\n",
    "                print(\"\\t%s: %0.3f\" % (k, v))\n",
    "\n",
    "            return metrics\n",
    "\n",
    "        else:\n",
    "            print(\"\\t# of predicted labels is {}, can not produce metrics. \\n\".format(np.unique(pred_labels)))\n",
    "            \n",
    "    # plot_clusters will visualize the clusters given predicted labels\n",
    "    def plot_clusters(self, estimator, X, labels, plot_kwargs):\n",
    "\n",
    "        palette = sns.color_palette('deep', np.unique(labels).max() + 1)\n",
    "        colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]\n",
    "\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=colors, **plot_kwargs)\n",
    "        plt.title('{} Clusters'.format(str(estimator.__name__)), fontsize=14)\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_all_clusters(self, estimators, labels, X, plot_kwargs):\n",
    "\n",
    "        fig = plt.figure()\n",
    "\n",
    "        for i, algorithm in enumerate(labels):\n",
    "\n",
    "            quotinent = np.divide(len(estimators), 2)\n",
    "\n",
    "            # Simple logic to decide row and column size of the figure\n",
    "            if isinstance(quotinent, int):\n",
    "                dim_1 = 2\n",
    "                dim_2 = quotinent\n",
    "            else:\n",
    "                dim_1 = np.ceil(quotinent)\n",
    "                dim_2 = 3\n",
    "\n",
    "            palette = sns.color_palette('deep',\n",
    "                                        np.unique(algorithm[estimators[i]['estimator'].__name__]).max() + 1)\n",
    "            colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in\n",
    "                      algorithm[estimators[i]['estimator'].__name__]]\n",
    "\n",
    "            plt.subplot(dim_1, dim_2, i + 1)\n",
    "            plt.scatter(X[:, 0], X[:, 1], c=colors, **plot_kwargs)\n",
    "            plt.title('{} Clusters'.format(str(estimators[i]['estimator'].__name__)), fontsize=8)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "estimators = [{'estimator': KMeans, 'args':(), 'kwargs':{'n_clusters': 4}}]\n",
    "\n",
    "unsupervised_learner = Unsupervised_AutoML(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised_learner.estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, performance_metrics = unsupervised_learner.fit_predict(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(performance_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kwargs = {'s': 12, 'linewidths': 0.1}\n",
    "unsupervised_learner.plot_clusters(KMeans,\n",
    "                                   X,\n",
    "                                   unsupervised_learner.predictions[0]['KMeans'],\n",
    "                                   plot_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=2000, centers=5, cluster_std=[1.7, 0.6, 0.8, 1.0, 1.2], random_state=220)\n",
    "\n",
    "# Plot sample data\n",
    "plt.scatter(X[:, 0], X[:, 1], **plot_kwargs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "estimators = [{'estimator': KMeans, 'args':(), 'kwargs':{'n_clusters': 4}}]\n",
    "\n",
    "unsupervised_learner = Unsupervised_AutoML(estimators)\n",
    "\n",
    "predictions, performance_metrics = unsupervised_learner.fit_predict(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kwargs = {'s': 12, 'linewidths': 0.1}\n",
    "unsupervised_learner.plot_clusters(KMeans,\n",
    "                                   X,\n",
    "                                   unsupervised_learner.predictions[0]['KMeans'],\n",
    "                                   plot_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "estimators = [{'estimator': DBSCAN, 'args':(), 'kwargs':{'eps': 0.5}}]\n",
    "\n",
    "unsupervised_learner = Unsupervised_AutoML(estimators)\n",
    "\n",
    "predictions, performance_metrics = unsupervised_learner.fit_predict(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kwargs = {'s': 12, 'linewidths': 0.1}\n",
    "unsupervised_learner.plot_clusters(DBSCAN,\n",
    "                                   X,\n",
    "                                   unsupervised_learner.predictions[0]['DBSCAN'],\n",
    "                                   plot_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "estimators = [{'estimator': AgglomerativeClustering, 'args':(), 'kwargs':{'n_clusters': 4, 'linkage': 'ward'}}]\n",
    "\n",
    "unsupervised_learner = Unsupervised_AutoML(estimators)\n",
    "\n",
    "predictions, performance_metrics = unsupervised_learner.fit_predict(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kwargs = {'s': 12, 'linewidths': 0.1}\n",
    "unsupervised_learner.plot_clusters(AgglomerativeClustering,\n",
    "                                   X,\n",
    "                                   unsupervised_learner.predictions[0]['AgglomerativeClustering'],\n",
    "                                   plot_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple automation of unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will create a list of algorithms to test\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth, SpectralClustering\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# bandwidth estimate for MeanShift algorithm to work properly\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.3, n_samples=100)\n",
    "\n",
    "estimators = [{'estimator': KMeans, 'args': (), 'kwargs': {'n_clusters': 5}},\n",
    "                         {'estimator': DBSCAN, 'args': (), 'kwargs': {'eps': 0.5}},\n",
    "                         {'estimator': AgglomerativeClustering, 'args': (), 'kwargs': {'n_clusters': 5, 'linkage': 'ward'}},\n",
    "                         {'estimator': MeanShift, 'args': (), 'kwargs': {'cluster_all': False, \"bandwidth\": bandwidth, \"bin_seeding\": True}},\n",
    "                         {'estimator': SpectralClustering, 'args': (), 'kwargs': {'n_clusters':5}},\n",
    "                         {'estimator': HDBSCAN, 'args': (), 'kwargs': {'min_cluster_size':15}}]\n",
    "\n",
    "\n",
    "unsupervised_learner = Unsupervised_AutoML(estimators)\n",
    "\n",
    "predictions, performance_metrics = unsupervised_learner.fit_predict(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kwargs = {'s': 12, 'linewidths': 0.1}\n",
    "unsupervised_learner.plot_all_clusters(estimators, unsupervised_learner.predictions, X, plot_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing high-dimensional datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wisconsin Breast Cancer Diagnostic Dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "pca = pca.fit_transform(df)\n",
    "\n",
    "plt.scatter(pca[:, 0], pca[:, 1], c=data.target, cmap=\"RdBu_r\", edgecolor=\"Red\", alpha=0.35)\n",
    "plt.colorbar()\n",
    "plt.title('PCA, n_components=2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process data.\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df)\n",
    "preprocessed_data = scaler.transform(df)\n",
    "scaled_features_df = pd.DataFrame(preprocessed_data, index=df.index, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "pca = pca.fit_transform(scaled_features_df)\n",
    "\n",
    "plt.scatter(pca[:, 0], pca[:, 1], c=data.target, cmap=\"RdBu_r\", edgecolor=\"Red\", alpha=0.35)\n",
    "plt.colorbar()\n",
    "plt.title('PCA, n_components=2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(verbose=1, perplexity=40, n_iter=4000)\n",
    "tsne = tsne.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsne[:, 0], tsne[:, 1], c=data.target, cmap=\"winter\", edgecolor=\"None\", alpha=0.35)\n",
    "plt.colorbar()\n",
    "plt.title('t-SNE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(verbose=1, perplexity=40, n_iter=4000)\n",
    "tsne = tsne.fit_transform(scaled_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsne[:, 0], tsne[:, 1], c=data.target, cmap=\"winter\", edgecolor=\"None\", alpha=0.35)\n",
    "plt.colorbar()\n",
    "plt.title('t-SNE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding simple components together to improve the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final version of Unsupervised_AutoML class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unsupervised_AutoML:\n",
    "\n",
    "    def __init__(self, estimators=None, transformers=None):\n",
    "        self.estimators = estimators\n",
    "        self.transformers = transformers\n",
    "        pass\n",
    "\n",
    "    def fit_predict(self, X, y=None, scaler=True, decomposer={'name': PCA, 'args':[], 'kwargs': {'n_components': 2}}):\n",
    "        \"\"\"\n",
    "        fit_predict will train given estimator(s) and predict cluster membership for each sample\n",
    "        \"\"\"\n",
    "\n",
    "        shape = X.shape\n",
    "        df_type = isinstance(X, pd.core.frame.DataFrame)\n",
    "\n",
    "        if df_type:\n",
    "            column_names = X.columns\n",
    "            index = X.index\n",
    "\n",
    "        if scaler == True:\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            scaler = StandardScaler()\n",
    "            X = scaler.fit_transform(X)\n",
    "\n",
    "            if df_type:\n",
    "                X = pd.DataFrame(X, index=index, columns=column_names)\n",
    "\n",
    "        if decomposer is not None:\n",
    "            X = decomposer['name'](*decomposer['args'], **decomposer['kwargs']).fit_transform(X)\n",
    "\n",
    "            if df_type:\n",
    "                if decomposer['name'].__name__ == 'PCA':\n",
    "                    X = pd.DataFrame(X, index=index, columns=['component_' + str(i + 1) for i in\n",
    "                                                              range(decomposer['kwargs']['n_components'])])\n",
    "                else:\n",
    "                    X = pd.DataFrame(X, index=index, columns=['component_1', 'component_2'])\n",
    "\n",
    "            # if dimensionality reduction is applied, then n_components will be set accordingly in hyperparameter configuration\n",
    "            for estimator in self.estimators:\n",
    "                if 'n_clusters' in estimator['kwargs'].keys():\n",
    "                    if decomposer['name'].__name__ == 'PCA':\n",
    "                        estimator['kwargs']['n_clusters'] = decomposer['kwargs']['n_components']\n",
    "                    else:\n",
    "                        estimator['kwargs']['n_clusters'] = 2\n",
    "\n",
    "        # This dictionary will hold predictions for each estimator\n",
    "        predictions = []\n",
    "        performance_metrics = {}\n",
    "\n",
    "        for estimator in self.estimators:\n",
    "            labels = estimator['estimator'](*estimator['args'], **estimator['kwargs']).fit_predict(X)\n",
    "            estimator['estimator'].n_clusters_ = len(np.unique(labels))\n",
    "            metrics = self._get_cluster_metrics(estimator['estimator'].__name__, estimator['estimator'].n_clusters_, X, labels, y)\n",
    "            predictions.append({estimator['estimator'].__name__: labels})\n",
    "            performance_metrics[estimator['estimator'].__name__] = metrics\n",
    "\n",
    "        self.predictions = predictions\n",
    "        self.performance_metrics = performance_metrics\n",
    "\n",
    "        return predictions, performance_metrics\n",
    "\n",
    "    # Printing cluster metrics for given arguments\n",
    "    def _get_cluster_metrics(self, name, n_clusters_, X, pred_labels, true_labels=None):\n",
    "        from sklearn.metrics import homogeneity_score, \\\n",
    "            completeness_score, \\\n",
    "            v_measure_score, \\\n",
    "            adjusted_rand_score, \\\n",
    "            adjusted_mutual_info_score, \\\n",
    "            silhouette_score\n",
    "\n",
    "        print(\"\"\"################## %s metrics #####################\"\"\" % name)\n",
    "        if len(np.unique(pred_labels)) >= 2:\n",
    "\n",
    "            silh_co = silhouette_score(X, pred_labels)\n",
    "\n",
    "            if true_labels is not None:\n",
    "\n",
    "                h_score = homogeneity_score(true_labels, pred_labels)\n",
    "                c_score = completeness_score(true_labels, pred_labels)\n",
    "                vm_score = v_measure_score(true_labels, pred_labels)\n",
    "                adj_r_score = adjusted_rand_score(true_labels, pred_labels)\n",
    "                adj_mut_info_score = adjusted_mutual_info_score(true_labels, pred_labels)\n",
    "\n",
    "                metrics = {\"Silhouette Coefficient\": silh_co,\n",
    "                           \"Estimated number of clusters\": n_clusters_,\n",
    "                           \"Homogeneity\": h_score,\n",
    "                           \"Completeness\": c_score,\n",
    "                           \"V-measure\": vm_score,\n",
    "                           \"Adjusted Rand Index\": adj_r_score,\n",
    "                           \"Adjusted Mutual Information\": adj_mut_info_score}\n",
    "\n",
    "                for k, v in metrics.items():\n",
    "                    print(\"\\t%s: %0.3f\" % (k, v))\n",
    "\n",
    "                return metrics\n",
    "\n",
    "            metrics = {\"Silhouette Coefficient\": silh_co,\n",
    "                       \"Estimated number of clusters\": n_clusters_}\n",
    "\n",
    "            for k, v in metrics.items():\n",
    "                print(\"\\t%s: %0.3f\" % (k, v))\n",
    "\n",
    "            return metrics\n",
    "\n",
    "        else:\n",
    "            print(\"\\t# of predicted labels is {}, can not produce metrics. \\n\".format(np.unique(pred_labels)))\n",
    "\n",
    "    # plot_clusters will visualize the clusters given predicted labels\n",
    "    def plot_clusters(self, estimator, X, labels, plot_kwargs):\n",
    "\n",
    "        palette = sns.color_palette('deep', np.unique(labels).max() + 1)\n",
    "        colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in labels]\n",
    "\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=colors, **plot_kwargs)\n",
    "        plt.title('{} Clusters'.format(str(estimator.__name__)), fontsize=14)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_all_clusters(self, estimators, labels, X, plot_kwargs):\n",
    "\n",
    "        fig = plt.figure()\n",
    "\n",
    "        for i, algorithm in enumerate(labels):\n",
    "\n",
    "            quotinent = np.divide(len(estimators), 2)\n",
    "\n",
    "            # Simple logic to decide row and column size of the figure\n",
    "            if isinstance(quotinent, int):\n",
    "                dim_1 = 2\n",
    "                dim_2 = quotinent\n",
    "            else:\n",
    "                dim_1 = np.ceil(quotinent)\n",
    "                dim_2 = 3\n",
    "\n",
    "            palette = sns.color_palette('deep',\n",
    "                                        np.unique(algorithm[estimators[i]['estimator'].__name__]).max() + 1)\n",
    "            colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in\n",
    "                      algorithm[estimators[i]['estimator'].__name__]]\n",
    "\n",
    "            plt.subplot(dim_1, dim_2, i + 1)\n",
    "            plt.scatter(X[:, 0], X[:, 1], c=colors, **plot_kwargs)\n",
    "            plt.title('{} Clusters'.format(str(estimators[i]['estimator'].__name__)), fontsize=8)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, MeanShift, estimate_bandwidth, SpectralClustering\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Necessary for bandwidth\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.1, n_samples=100)\n",
    "\n",
    "estimators = [{'estimator': KMeans, 'args': (), 'kwargs': {'n_clusters': 5}},\n",
    "                         {'estimator': DBSCAN, 'args': (), 'kwargs': {'eps': 0.3}},\n",
    "                         {'estimator': AgglomerativeClustering, 'args': (), 'kwargs': {'n_clusters': 5, 'linkage': 'ward'}},\n",
    "                         {'estimator': MeanShift, 'args': (), 'kwargs': {'cluster_all': False, \"bandwidth\": bandwidth, \"bin_seeding\": True}},\n",
    "                         {'estimator': SpectralClustering, 'args': (), 'kwargs': {'n_clusters':5}},\n",
    "                         {'estimator': HDBSCAN, 'args': (), 'kwargs': {'min_cluster_size':15}}]\n",
    "\n",
    "unsupervised_learner = Unsupervised_AutoML(estimators)\n",
    "\n",
    "predictions, performance_metrics = unsupervised_learner.fit_predict(X, y, decomposer=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
