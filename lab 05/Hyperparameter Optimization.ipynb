{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def branin(x):\n",
    "\n",
    "    # Branin function has 2 dimensions and it has 3 global mimima\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "\n",
    "    # Global minimum is f(x*)=0.397887 at points (-pi, 12.275), (pi,2.275) and (9.42478, 2.475)\n",
    "\n",
    "    # Recommended values of a, b, c, r, s and t for Branin function\n",
    "    a = 1\n",
    "    b = 5.1 / (4 * np.pi**2)\n",
    "    c = 5. / np.pi\n",
    "    r = 6.\n",
    "    s = 10.\n",
    "    t = 1 / (8 * np.pi)\n",
    "\n",
    "    # Calculating separate parts of the function first for verbosity\n",
    "    p1 = a * (x2 - (b * x1**2) + (c * x1) - r)**2\n",
    "    p2 = s * (1-t) * np.cos(x1)\n",
    "    p3 = s\n",
    "\n",
    "    # Calculating result\n",
    "    ret = p1 + p2 + p3\n",
    "\n",
    "    return ret\n",
    "\n",
    "# minimize function from scipy.optimize will minimize a scalar function with one or more variables\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "x = [5.6, 3.2]\n",
    "\n",
    "res = minimize(branin, x)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Hyperparameters\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 20, 50, 100],\n",
    "                'penalty': ['l1', 'l2']}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "n_folds = 5\n",
    "estimator = GridSearchCV(log_reg,param_grid, cv=n_folds)\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "estimator.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "# Hyperparameters\n",
    "param_grid = {'C': sp_randint(1, 100),\n",
    "                'penalty': ['l1', 'l2']}\n",
    "\n",
    "n_iter_search = 20\n",
    "n_folds = 5\n",
    "estimator = RandomizedSearchCV(log_reg, param_distributions=param_grid, n_iter=n_iter_search, cv=n_folds)\n",
    "\n",
    "estimator.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(estimator.cv_results_)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['mean_test_score'] == df['mean_test_score'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(C=10, tol=0.00001)\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "from time import time\n",
    "start = time()\n",
    "log_reg.fit(X, Y)\n",
    "end = time()\n",
    "print(\"Time: {}\".format(end - start))\n",
    "# Time: 0.0009272098541259766\n",
    "\n",
    "log_reg.set_params(C=20)\n",
    "# LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
    "# intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "# penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "# verbose=0, warm_start=False)\n",
    "\n",
    "start = time()\n",
    "log_reg.fit(X, Y)\n",
    "end = time()\n",
    "print(\"Time: {}\".format(end - start))\n",
    "# Time: 0.0012941360473632812"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(C=10, solver='sag', warm_start=True, max_iter=10000)\n",
    "\n",
    "start = time()\n",
    "log_reg.fit(X, Y)\n",
    "end = time()\n",
    "print(\"Time: {}\".format(end - start))\n",
    "# Time: 0.043714046478271484\n",
    "\n",
    "log_reg.set_params(C=20)\n",
    "\n",
    "start = time()\n",
    "log_reg.fit(X, Y)\n",
    "end = time()\n",
    "print(\"Time: {}\".format(end - start))\n",
    "# Time: 0.020781755447387695"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian-based hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smac.facade.func_facade import fmin_smac\n",
    "\n",
    "x, cost, _ = fmin_smac(func=branin, # function\n",
    "                           x0=[3.2, 4.5], # default configuration\n",
    "                           bounds=[(-5, 10), (0, 15)], # limits\n",
    "                           maxfun=500, # maximum number of evaluations\n",
    "                           rng=3) # random seed\n",
    "\n",
    "\n",
    "print(x, cost)\n",
    "# [3.07419145 2.39022223] 0.4235428462537083"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Importomg ConfigSpace and different types of parameters\n",
    "from smac.configspace import ConfigurationSpace\n",
    "from ConfigSpace.hyperparameters import CategoricalHyperparameter, \\\n",
    "    UniformFloatHyperparameter, UniformIntegerHyperparameter\n",
    "from ConfigSpace.conditions import InCondition\n",
    "\n",
    "# Import SMAC-utilities\n",
    "from smac.tae.execute_func import ExecuteTAFuncDict\n",
    "from smac.scenario.scenario import Scenario\n",
    "from smac.facade.smac_facade import SMAC\n",
    "\n",
    "# Creating configuration space.\n",
    "# Configuration space will hold all of your hyperparameters\n",
    "cs = ConfigurationSpace()\n",
    "\n",
    "# Defining hyperparameters and range of values that they can take\n",
    "learning_rate = UniformFloatHyperparameter(\"learning_rate\", 0.001, 0.1, default_value=0.1)\n",
    "n_estimators = UniformIntegerHyperparameter(\"n_estimators\", 100, 200, default_value=100)\n",
    "\n",
    "# Adding hyperparameters to configuration space\n",
    "cs.add_hyperparameters([learning_rate, n_estimators])\n",
    "\n",
    "# Loading data set\n",
    "wbc_dataset = datasets.load_breast_cancer()\n",
    "\n",
    "# Creating function to cross validate XGBoost classifier given the configuration space\n",
    "def xgboost_from_cfg(cfg):\n",
    "    \"\"\" Creates a XGBoost based on a configuration and evaluates it on the\n",
    "    Wisconsin Breast Cancer-dataset using cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    cfg: Configuration (ConfigSpace.ConfigurationSpace.Configuration)\n",
    "        Configuration containing the parameters.\n",
    "        Configurations are indexable!\n",
    "    Returns:\n",
    "    --------\n",
    "    A crossvalidated mean score for the svm on the loaded data-set.\n",
    "    \"\"\"\n",
    "\n",
    "    cfg = {k: cfg[k] for k in cfg if cfg[k]}\n",
    "\n",
    "    clf = XGBClassifier(**cfg, eval_metric='auc', early_stopping_rounds=50, random_state=42)\n",
    "\n",
    "    scores = cross_val_score(clf, wbc_dataset.data, wbc_dataset.target, cv=5)\n",
    "\n",
    "    return 1 - np.mean(scores) # Minimize!\n",
    "\n",
    "\n",
    "# Creating Scenario object\n",
    "scenario = Scenario({\"run_obj\": \"quality\",\n",
    "                     \"runcount-limit\": 200, # maximum function evaluations\n",
    "                     \"cs\": cs, # configuration space\n",
    "                     \"deterministic\": \"true\"\n",
    "                     })\n",
    "\n",
    "\n",
    "\n",
    "# SMAC object handles bayesian optimization loop\n",
    "print(\"Please wait until optimization is finished\")\n",
    "smac = SMAC(scenario=scenario, rng=np.random.RandomState(42),\n",
    "        tae_runner=xgboost_from_cfg)\n",
    "\n",
    "incumbent = smac.optimize()\n",
    "\n",
    "# Let's see the best performing hyperparameter values\n",
    "print(incumbent)\n",
    "# Configuration:\n",
    "# learning_rate, Value: 0.08815217130807515\n",
    "# n_estimators, Value: 196\n",
    "\n",
    "# You can see the errpr rate of optimized hyperparameters\n",
    "inc_value = xgboost_from_cfg(incumbent)\n",
    "\n",
    "print(\"Optimized Value: %.2f\" % (inc_value))\n",
    "# 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_1 = []\n",
    "param_2 = []\n",
    "costs = []\n",
    "\n",
    "for k,v in smac.runhistory.config_ids.items():\n",
    "    param_1.append(k._values['learning_rate'])\n",
    "    param_2.append(k._values['n_estimators'])\n",
    "    costs.append(smac.runhistory.cost_per_config[v])\n",
    "\n",
    "print(len(param_1), len(param_2), len(costs))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "sc = plt.scatter(param_1, param_2, c=costs)\n",
    "\n",
    "plt.colorbar(sc)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
